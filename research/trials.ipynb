{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract Data From the PDF File\n",
    "def load_pdf_file(data):\n",
    "    loader= DirectoryLoader(data,\n",
    "                            glob=\"*.pdf\",\n",
    "                            loader_cls=PyPDFLoader)\n",
    "\n",
    "    documents=loader.load()\n",
    "\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_data=load_pdf_file(data='D:\\Projects\\GenAi_Medical_Chatbot\\Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the Data into Text Chunks\n",
    "def text_split(extracted_data):\n",
    "    text_splitter=RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)\n",
    "    text_chunks=text_splitter.split_documents(extracted_data)\n",
    "    return text_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Text Chunks 5860\n"
     ]
    }
   ],
   "source": [
    "text_chunks=text_split(extracted_data)\n",
    "print(\"Length of Text Chunks\", len(text_chunks))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "def download_hugging_face_embeddings():\n",
    "    embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = download_hugging_face_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length 384\n"
     ]
    }
   ],
   "source": [
    "query_result = model.encode(\"Hello world\")\n",
    "print(\"Length\", len(query_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "PINECONE_API_KEY=os.environ.get('PINECONE_API_KEY')\n",
    "OPENAI_API_KEY=os.environ.get('OPENAI_API_KEY')\n",
    "HUGGINGFACEHUB_API_TOKEN=os.environ.get('HUGGINGFACEHUB_API_TOKEN')\n",
    "#print(HUGGINGFACEHUB_API_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pinecone import Pinecone  # Updated import for Pinecone 6.0.2\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Initialize Pinecone (different in 6.0.2)\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "\n",
    "# Connect to your existing index\n",
    "index_name = \"medibot\"\n",
    "index = pc.Index(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5860\n"
     ]
    }
   ],
   "source": [
    "print(len(text_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [05:46<00:00,  5.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully embedded and upserted 5860 document chunks to Pinecone\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "'''# Embed each chunk and upsert the embeddings into your Pinecone index.\n",
    "\n",
    "def embed_and_upsert(text_chunks, embeddings_model):\n",
    "    batch_size = 100  # Adjust as needed\n",
    "    \n",
    "    for i in tqdm(range(0, len(text_chunks), batch_size)):\n",
    "        # Get the batch of documents\n",
    "        batch = text_chunks[i:i+batch_size]\n",
    "        \n",
    "        # Get texts to embed\n",
    "        texts = [doc.page_content for doc in batch]\n",
    "        \n",
    "        # Generate embeddings\n",
    "        embeds = embeddings_model.embed_documents(texts)\n",
    "        \n",
    "        # Get metadata \n",
    "        metadatas = [{\"text\": doc.page_content, \n",
    "                     \"source\": doc.metadata.get(\"source\", \"\"),\n",
    "                     \"page\": doc.metadata.get(\"page\", 0)} for doc in batch]\n",
    "        \n",
    "        # Create IDs\n",
    "        ids = [f\"doc_{i+j}\" for j in range(len(batch))]\n",
    "        \n",
    "        # Create records in the format expected by Pinecone 6.0.2\n",
    "        records = [\n",
    "            {\"id\": ids[j], \n",
    "             \"values\": embeds[j], \n",
    "             \"metadata\": metadatas[j]\n",
    "            } for j in range(len(batch))\n",
    "        ]\n",
    "        \n",
    "        # Upsert to Pinecone\n",
    "        index.upsert(vectors=records)\n",
    "        \n",
    "    return f\"Successfully embedded and upserted {len(text_chunks)} document chunks to Pinecone\"\n",
    "result = embed_and_upsert(text_chunks, embeddings)\n",
    "print(result)'\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Existing index \n",
    "\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "# Embed each chunk and upsert the embeddings into your Pinecone index.\n",
    "docsearch = PineconeVectorStore.from_existing_index(\n",
    "    index_name=index_name,\n",
    "    embedding=embeddings\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = docsearch.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_docs = retriever.invoke(\"What is Dandruff?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='doc_1223', metadata={'page': 139.0, 'source': 'D:\\\\Projects\\\\GenAi_Medical_Chatbot\\\\Data\\\\Medical_book.pdf'}, page_content='its own tissues.\\nChemotherapy—The treatment of diseases, usual-\\nly cancer, with drugs (chemicals).\\nHair follicles—Tiny organs in the skin, each one of\\nwhich grows a single hair.\\nLupus erythematosus —An autoimmune disease\\nthat can damage skin, joints, kidneys, and other\\norgans.\\nRingworm—A fungal infection of the skin, usually\\nknown as tinea corporis.\\nSystemic—Affecting all or most parts of the body.\\ntime, minoxidil produces satisfactory results in about one'),\n",
       " Document(id='doc_1219', metadata={'page': 138.0, 'source': 'D:\\\\Projects\\\\GenAi_Medical_Chatbot\\\\Data\\\\Medical_book.pdf'}, page_content='Alopecia\\nTop of balding male’s head.(Photograph by Kelly A. Quin.\\nReproduced by permission.)\\nGEM - 0001 to 0432 - A  10/22/03 1:42 PM  Page 125'),\n",
       " Document(id='doc_1215', metadata={'page': 138.0, 'source': 'D:\\\\Projects\\\\GenAi_Medical_Chatbot\\\\Data\\\\Medical_book.pdf'}, page_content='pulling it out to having it killed off by cancer chemo-\\ntherapy. Some causes are considered natural, while oth-\\ners signal serious health problems. Some conditions are\\nconfined to the scalp. Others reflect disease throughout\\nthe body. Being plainly visible, the skin and its compo-\\nnents can provide early signs of disease elsewhere in the\\nbody.\\nOftentimes, conditions affecting the skin of the scalp\\nwill result in hair loss. The first clue to the specific cause')]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from langchain_openai import OpenAI\n",
    "\n",
    "#llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0.4, max_tokens=500)\n",
    "\n",
    "from langchain_community.llms import HuggingFaceHub\n",
    "\n",
    "llm = HuggingFaceHub(\n",
    "    repo_id=\"google/flan-t5-base\",\n",
    "    model_kwargs={\"temperature\": 0.4, \"max_length\": 1024},\n",
    "    huggingfacehub_api_token=HUGGINGFACEHUB_API_TOKEN\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that\"\n",
    "    \"Sorry, That information is not availabe in the materila provided,Try with different keywords.\" \n",
    "    \"Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Projects\\GenAi_Medical_Chatbot\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hyperactivity\n"
     ]
    }
   ],
   "source": [
    "response = rag_chain.invoke({\"input\": \"what is diziness?\"})\n",
    "print(response[\"answer\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
